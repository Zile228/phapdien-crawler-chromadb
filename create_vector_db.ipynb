{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "''' \n",
        "This script runs on Google Colab and performs the following tasks:\n",
        "1. Load and preprocess Vietnamese legal documents in HTML format.\n",
        "2. Use BeautifulSoup for parsing since UnstructuredHTMLLoader and BSHTMLLoader did not return desired results.\n",
        "3. Tokenize Vietnamese text using PyVi's ViTokenizer because Vietnamese has special syntax rules,\n",
        "   and the bkai bi-encoder requires word-segmented input before embedding.\n",
        "4. Embed documents using HuggingFace's Vietnamese Bi-Encoder model.\n",
        "5. Store and persist embeddings in ChromaDB, saving the database to Google Drive.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvm8ZgKgux0l",
        "outputId": "ce5d12cf-9b36-4057-c523-994dfad4ae55"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -qU langchain-huggingface\n",
        "!pip install -qU \"langchain-chroma>=0.1.2\"\n",
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psm55vVrve4v"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import chromadb\n",
        "from pyvi import ViTokenizer\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu2p5x8sRisR"
      },
      "outputs": [],
      "source": [
        "# Define folder path containing HTML files\n",
        "folder_path = \"/content/vbpl\"\n",
        "html_files = [f for f in os.listdir(folder_path) if f.endswith(\".html\")]\n",
        "\n",
        "documents = []  # List to store parsed documents\n",
        "\n",
        "# Process each HTML file\n",
        "for file_name in html_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Open and read the file\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        soup = BeautifulSoup(file, \"html.parser\") # Parse HTML content\n",
        "        text_content = soup.get_text() # Extract plain text\n",
        "        text_content = re.sub(r'\\n+', '\\n', text_content)  # Remove excessive newlines\n",
        "        text_content = ViTokenizer.tokenize(text_content)  # Tokenize Vietnamese text\n",
        "        '''According to bkai-foundation-models/vietnamese-bi-encoder, the input text has to be word-segmented. Hence the ViTokenizerViTokenizer'''\n",
        "    # Create a Document object with metadata\n",
        "    doc = Document(page_content=text_content, metadata={\"file_path\": file_path})\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "# Split documents into smaller chunks for embedding\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=20)\n",
        "split_docs = splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZFqCgaNMRbti"
      },
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name='bkai-foundation-models/vietnamese-bi-encoder')\n",
        "\n",
        "# Define ChromaDB storage path\n",
        "chroma_db_path = \"/content/vectordb\"\n",
        "\n",
        "# Create a Chroma vector database and add document embeddings\n",
        "vector_db = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings)\n",
        "vector_db.add_documents(split_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_GIVwFfQBoL",
        "outputId": "1159c7f5-2fd6-4a86-ae21-60881447a856"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to save the database\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Move the zipped database to Google Drive\n",
        "!zip -r vectordb.zip vectordb\n",
        "!mv vectordb.zip /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
